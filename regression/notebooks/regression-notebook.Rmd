---
title: Regression problem
subtitle: The release year prediction of the songs from the timbre features.
author: "Mateusz Bary≈Ça & Agata Wytrykowska"
bibliography: bibliography.bib
link-citations: yes
nocite: '@*'
output:
    html_document:
        toc: true
        toc_float: true
        number_sections: true
        theme: united
        df_print: paged
---

# Description of data and problem analyzed

Audio features play a significant role in the prediction of the song release year. The Million Song Dataset is a free collection of million music tracks consists. Based on this dataset a subset has been created that consists mostly the songs between 1922 to 2011. The majority of the observations relate to the 2000s [@Bertin-Mahieux2011]. We downloaded it from the Machine Learning Repository [@Dua:2019].

```{r, echo = FALSE}
pacman::p_load(data.table, tidyverse, knitr,
               GGally, ggcorrplot, tidymodels,
               randomForest, Rcpp, ranger,
               kableExtra, tree, rpart, caret,
               VSURF)
experiment <- FALSE
train_models <- FALSE
```

There are 515345 observations in the dataset. 90 attributes describe each and single observation. All of the attributes are real number and correspond to the timbre either average or covariance. The Echo Nest API is used in order to extract the features from the timbre which must be an additional service. In overall, timbre-related features describes spectogram patches. Considering the particular dimensions of the 12-dimension timbre vector they represent the average loudness of the segment, with the second the brightness is described, the third relate to the flatness of a sound and the fourth to the sounds with stronger attack [@Bertin-Mahieux2013].

## Data Preprocessing

Data preprocessing is told to take around 80 % of the machine learning project. Analyzed dataset has a lot of observations that makes it hard to prototype quickly. We prepared a subsample of this dataset that was used for this purpose, in the very early stage of the projects.

## Reading data

We begin with reading the data.

```{r, echo = FALSE}
source('..\\src\\utils\\fit_table.r')
source("..\\src\\utils\\t_test_across_centuries.R")
source('..\\src\\models\\getRegressionMetrics.R')
```

```{r}
if (experiment){
  raw_data <- read.csv2('..\\data\\raw\\YearPredictionMSD_sample.txt')
} else{
  raw_data <- read.csv('..\\data\\raw\\YearPredictionMSD.txt', header = FALSE)
}
raw_data %>% fit_table()
```

The first column in the dataset correspond to the year that will be predicted from the features. With the MSD many tasks can be addressed. Interestingly, the song release year brings many practical applications. There are not a lot of research papers that relate to this prediction problem. Only audio features are used in this problem which is justified with the fact that listeners are often fond of music from certain periods of their lives. Another reason might be the evolution of timbre across years.

## Renaming columns

We rename the first column for the sake of the modelling purposes. 

```{r}
raw_data <- raw_data %>%
        rename(year = V1) %>%
        mutate(year = as.numeric(year))
raw_data %>% fit_table()
```

## Scaling

The authors in the main article mapped the year values using linear mapping to [0,1] range [@Bertin-Mahieux2011]. We follow the convention in the analysis below.

```{r}
range_to_0_1 <- function(vec) {
  (vec - min(vec)) / (max(vec) - min(vec))
}

raw_data$year_scaled <- range_to_0_1(raw_data$year)
```

We add the column to the raw dataset and mark that this is scaled variable.

## Splitting covariates

We needed to investigate what the timbre in the sound analysis actually means. It is described as everything that can be differentiated between two different tracks by the listener having kept the same pitch, spatial location and loudness and refers to the perceptual quality of a sound and color. "Brightness", "clarity", "harshness", "fullness" and "noisiness" are frequently used adjectives that are associated with the timbre word [@Allen2018].

```{r}
timbre_average_data <- raw_data %>%
        select(V2:V13)
timbre_average_data %>% fit_table()
```

Covariance is taken over all segments, where each segment is described by 12-dimensional vector.

```{r}
timbre_covariance_data <- raw_data %>%
        select(V14:V91)
timbre_covariance_data %>% fit_table()
```

## Handling missing values

We check whether there are missing values in the dataset.

```{r}
raw_data %>%
        summarise(across(everything(), ~sum(is.na(.))))
```

There are no missing values across columns and in the whole dataset.

# Descriptive analyses of the data

Having prepared the dataset we can easily follow the analysis. In the next points visualize the dataset and include both target and features on which the model will be trained.

## Distributions plots

We begin with distributions of the year variable.

```{r}
raw_data %>%
    ggplot(aes(x = year)) +
    geom_histogram(aes(y = ..density..),
                   bins = 50, alpha = 0.4, fill = "pink", col = "gray30") +
    geom_density(alpha = .2, fill = "blueviolet") +
    labs(title = "Distribution of the outcome variable - density and histogram",
         caption = "source: produced with ggplot package") +
    theme(plot.title = element_text(hjust = 0.5, size = 12))
```

We easily spot that the density of the target variable year is heavily skewed.

## Advanced visualizations

We proceed with more advanced visualization.

### Number of songs across centuries

For the first plot we have decided to squeeze the year variable based on the century because the distribution of the songs across years is highly non-uniform.

```{r}
timbre_average_data %>%
    mutate(year = raw_data$year) %>%
    mutate(century = if_else(year <= 2000, 'XX', 'XXI')) %>%
    group_by(century) %>%
    count() %>%
    ggplot(aes(x = century, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle('The songs count across centuries') +
    labs(caption = "source: produced with ggplot package",
         x = 'Century',
         y = 'Count') +
    theme_minimal() +
    theme(legend.position = 'bottom')  +
    guides(fill = guide_legend(nrow = 2,byrow = TRUE)) +
    scale_fill_grey() +
    theme(plot.title = element_text(hjust = 0.5, size = 12))
```

We see that the count of songs increase with the centuries, although, there are more years in the dataset, in XX century.

### Loudness across decades

After getting to know some feature names we can visualize the one that is most intuitive for us, loudness, in the form of boxplot across decades.

```{r}
timbre_average_data %>%
    mutate(year = raw_data %>% pull(year)) %>%
    mutate(decade = as.factor(floor(year/10)*10)) %>%
    rename(loudness = V2) %>%
    select(loudness, decade) %>%
    ggplot(aes(x= decade, y = loudness, group=decade, fill = decade)) +
    geom_boxplot() +
    labs(title = "Loudness across decades",
         caption = "source: produced with ggplot package") +
    guides(fill = "none", group = "none") +
    theme(plot.title = element_text(hjust = 0.5, size = 12))
```

Increase in loudness is spotted across decades. Nowadays louder music is produced.

### Correlation matrix

An important area that should not be neglected in the process of building machine learning model is assessing the correlations.

```{r}
corr_average <- timbre_average_data %>%
        mutate(year = raw_data$year) %>%
        cor()

ggcorrplot(corr_average, hc.order = TRUE, type = "lower", lab = TRUE) +
    labs(title = "Correlation between timbre average data and the outcome variable",
         caption = "source: produced with ggplot package") +
    theme(plot.title = element_text(hjust = 0.5, size = 12))
```

Based on the correlation matrix we spot two variables that might be important in predicting the song release year V2 and V7. V7 is negatively correlated with the year which means that the higher the variable the lower the year.

```{r}
corr_covariance <- timbre_covariance_data %>%
        mutate(year = raw_data$year) %>%
        cor()
```

Because there are a lot of columns inside the timbre covariance matrix we choose to visualize only the ones that are greater than chosen level of correlation.

```{r}
high_corr_columns <- corr_covariance %>%
        as_tibble() %>%
        mutate(row_name = c(timbre_covariance_data %>% names(), "year")) %>%
        relocate(row_name) %>%
        pivot_longer(-row_name) %>%
        filter(name == 'year') %>%
        filter(value > 0.05) %>%
        pull(row_name)
```

The threshold is set to extremely low value, only 0.05, because of the overall small of correlation with the target variable.

```{r}
corr_covariance <-  timbre_covariance_data %>%
        mutate(year = raw_data$year) %>%
        select(all_of(high_corr_columns)) %>%
        cor()
```

```{r}
ggcorrplot(corr_covariance,
           hc.order = TRUE,
           type = "lower",
           lab = TRUE,
           method = 'circle',
           lab_size = 3) +
  labs(title = "Correlation between timbre covariance data and the outcome variable",
       caption = "source: produced with ggplot package") +
  theme(plot.title = element_text(hjust = 0.5, size = 12))
```

The highest association with the target variable is spotted between variable V21, V74, V15 and V40. What is more the multicollinearity might be spotted for variable V15 and V21 with the correlation value 0.48.

## Statistical analysis

After visualization step we are checking the differences in means across centuries for the most correlated variables.

```{r}
t_test_test_across_centuries(timbre_average_data %>%
                                     mutate(year = raw_data$year),
                             'V2')
t_test_test_across_centuries(timbre_average_data %>%
                                     mutate(year = raw_data$year),
                             'V7')
t_test_test_across_centuries(timbre_covariance_data %>%
                               mutate(year = raw_data$year),
                             'V21')
t_test_test_across_centuries(timbre_covariance_data %>%
                                     mutate(year = raw_data$year),
                             'V15')
t_test_test_across_centuries(timbre_covariance_data %>%
                                     mutate(year = raw_data$year),
                             'V74')
```

We show that across the centuries averages differ for the variables that are the most correlated with the target variable. It might be an indicator that these variables will play a significant role in predicting the song release year. On the other hand, with the dataset with thousands of observations even small differences might be statistically significant.

# Variable transformation

In this step we embrace creating interactions between the most influential variables accordingly to the previous step.

## Interactions

Since we deal with only numeric variables we do not need to perform dummification step. We focus on the interactions between most correlated variables.

```{r}
formula_for_interactions <- year_scaled ~ V2 + V7 + V21 + V15 + V74

rec <- recipe(formula_for_interactions, data = raw_data ) # is it correct?

interact_terms <- rec %>%
  step_interact(terms = ~ starts_with("V"):starts_with("V"))

interact_prep <- prep(interact_terms, training = raw_data)

transformed_data <- bake(interact_prep, raw_data)

transformed_data %>% names()
```

Final transformed data used in the modelling consist of 16 dimensions. They are a result of variable transformation step.

# Variable selection

Having chosen the dimensions we investigate two packages varSelRF [@varselrf] and VSURF [@genuer:hal-01251924] for variable selection step. We choose them mainly because of their dependency on the randomForest package that was used during the classes. The latter one is not implemented for regression problem. Hence, varSelfRF will be used which is more complicated procedure. It consists of three steps: thresholding, interpretation and prediction step. Thresholding is based on variable importance. The interpretation embeds Random Forest models starting with the one built only on the variables chosen in the previous step. Next, the stepwise selection procedure is applied. Last but not least prediction is considered.

 We will present the code for this section and evaluate its performance in the comparison of the trained machine learning models.

# Training/test data division and resampling

## Splitting dataset

The following train / test split is respected. First 463 715 observations should be used for the training purposes while the remaining 51 630 are the observations on which the regression model will be tested.

The dataset authors add that thanks to that split 'producer effect' will be avoided. Next, we confirm that the remaining attributes correspond to the timbre covariates.

```{r}
if (experiment){
  train_data <- transformed_data %>% slice_head(n = 890)
  test_data <- transformed_data %>% slice_tail(n = 110)
} else{
  train_data <- transformed_data %>% slice_head(n = 463715)
  test_data <- transformed_data %>% slice_tail(n = 51630)
}
```

We considered all the variables in the dataset, however the problem appear to be unfeasible. We experienced both memory and time complexity. We try to omit the obstacles with the large size of the dataset. We choose only these dimensions shown in descriptive analysis that are correlated with the target above the manually specified threshold.

# Different machine learning methods

Followingly, after preparing the final dimensions and splitting the dataset into the train and test data we build several machine learning models.

## Basic tree model

In the formula below we state that all the finally transformed and chosen variables will be used for training.

```{r}
model_formula <- year_scaled ~ .
```

We start with the basic tree model - `tree()`. It uses the *binary recursive algorithm*. Splits are generated until the criterium that relates to node impurity is not met.

```{r}
basic_tree <- tree(model_formula, train_data)
```

This basic tree might serve as a baseline model for more advanced techniques.

```{r}
summary(basic_tree)
```

As presented above the model chosen only two variables for the final prediction. Both of them include loudness.

```{r}
plot(basic_tree)
text(basic_tree, pretty = 0)
```

Interestingly, there are only three terminal nodes. It might relate to the skewness of the target variable. Again, it can be described as naive, baseline model.


## Optimized tree model with rpart

Package rpart offer different way of handling surrogate variables. We want to optimize  `minsplit`. It relates to the number of observations that need to belong to the node before the split is considered. We neglect `maxdepth` because of already contrained number of final variables included in the model.

```{r, results='hide'}
hyper_vec <- expand.grid(
        minsplit = seq(2, 8, 1)
)
```

The values in the grid are chosen so that they relate to the data that we are using in this problem.

```{r, results='hide'}
vec_models <- list()
for (comb in 1:nrow(hyper_vec)) {

  cat(comb, "/", nrow(hyper_vec), "\n", sep = "")

  minsplit <- hyper_vec$minsplit[comb]

  set.seed(9438 + comb)

  vec_models[[comb]] <- rpart(
          formula = model_formula,
          data    = train_data,
          method  = "anova",
          control = list(minsplit = minsplit)
  )
}
```

Now models with the lowest error can be easily chosen. Therefore,
we use the functions that were introduced during the classes get_cp and get_min_error.

```{r, echo = FALSE}
get_cp <- function(x) {
  min <- which.min(x$cptable[, "xerror"])
  cp  <- x$cptable[min, "CP"]
  return(cp)
}

get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"]
  return(xerror)
}
```

Last but not least, we choose top three performers out of 7 different models.

```{r, echo = FALSE}
hyper_vec %>%
        mutate(
                cp    = purrr::map_dbl(vec_models, get_cp),
                error = purrr::map_dbl(vec_models, get_min_error)
        ) %>%
        arrange(error) %>%
        top_n(-3, wt = error)
```

Let's build tree with the parameters that correspond to the lowest prediction error on the testing set.

```{r }
optimized_tree <- rpart(
        formula = model_formula,
        data    = train_data,
        method  = "anova",
        control = list(minsplit = 5, cp = 0.01)
)
```

Optimized tree is the one with parameters that correspond to the lowest error.

## XGBoost

As the next step, we use XGBoost.

```{r results='hide'}
trainctrl <- trainControl(verboseIter = TRUE,
                          method = "cv",
                          number = 10)

if (train_models){
    xgb_model <- train(model_formula,
        data = train_data,
        method = "xgbTree",
        trControl = trainctrl,
        preProcess = c("center", "scale"))
} else{
  xgb_model <- readRDS("..\\models\\xgb_model.rds")
}
```

What is more, we additionally center and scale the predictors. Cross-validation with 10 folds is a part of training this model.

## VSURF

Variable selection occurred to be the most challenging process. It seems to be computationally infeasible. We limit the training data to sample 10000 observations.

```{r, results='hide'}
if (experiment){
  data_for_vsurf <- train_data %>% sample_n(100)
} else{
  data_for_vsurf <- train_data %>% sample_n(10000)
}

if (train_models){
  vsurf <- VSURF(x = data_for_vsurf %>% select(-year_scaled),
                 y = data_for_vsurf %>%  pull(year_scaled),
                 ntree = 100, nfor.thres = 13,
                 parallel = 4,
                 nfor.interp = 5, nfor.pred = 3)
} else{
  vsurf <- readRDS("..\\models\\vsurf_model.rds")
}

```

The result of this step already trains the first model.

# Comparison

In this step we compare models built in the report and their accuracy on the test data. We will use script from the classes - getRegressionMetrics.

## Regression metrics

We add yielding the results in a form of a readable table.

```{r }
performance_comparison <- bind_cols(tibble(tree         =  predict(basic_tree,  test_data),
                 vsurf = predict(vsurf, test_data) %>% pull(pred),
                 xgb = predict(xgb_model, test_data),
                 optimized_tree         =  predict(optimized_tree,  test_data))) %>%
        map_dfr(getRegressionMetrics, real = test_data$year_scaled, .id = "model")

performance_comparison %>% fit_table()
```

XGBoost is the best performing model on the test dataset.

## XGBoost final predictions

We present the results in a form of scatter plot of true values vs the corresponding predictions for the top model achieved in this project.

```{r}
y_pred <- xgb_model %>% predict(test_data)

dt <- tibble(
  y = test_data$year_scaled,
  y_pred = as.numeric(y_pred)
)
dt %>% glimpse()

dt %>%
  ggplot(aes(x = y, y = y_pred)) +
  geom_point() +
  geom_smooth(method = lm, se = T) +
  labs(x = "True values", y = "Predictions")
```

The model mostly predicts high values of the target variable. Nevertheless, we improve the results in comparison to the baseline model. Accordingly to the article this improvement is already significant.

# Summary and conclusions

First of all, for the first time we have been modelling on dataset of this size.
We needed to take appropriate measures in order to experiment faster for instance
sampling 1000 observations and working only on them.

Secondly, the names of the variables were not provided. We managed to find the naming for only few of them. We were not aware how the timbre might influence the outcome variable and haven't any intuition towards the variables that might be important. Hence, we focused on quantitative measures such as correlations or statistical tests. Nevertheless, the visualizations also occurred to be informative.

What is more, the variable selection process was an opportunity to learn more about
how to choose packages. During the notebook production we have already known
what libraries were introduced during the classes (randomForest), what machine learning problem we analyze (regression) and what are the methods that it needs to implement (predict). Considering these three constraints we have chosen one that suits all of the aforementioned needs.

Last but not least, it has been a fruitful experience to build different machine learning models. However, the limited resources of local machines suggest that using Apache Spark in R or cloud solutions with more power would be beneficial.

# References