---
title: Regression problem
subtitle: The release year prediction of the songs from the timbre features.
author: "Mateusz Bary≈Ça & Agata Wytrykowska"
bibliography: bibliography.bib
link-citations: yes
nocite: '@*'
output:
html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: united
    df_print: paged
---

# Description of data and problem analyzed


Audio features play a significant role in the prediction of the song release year. The Million Song Dataset is a free collection of million music tracks consists. Based on this dataset a subset has been created that returned mostly the songs between 1922 to 2011. Most of the observations relate to the  2000s [@Bertin-Mahieux2011]. We downloaded it from the Machine Learning Repository [@Dua:2019].

```{r, echo = FALSE}
pacman::p_load(data.table, tidyverse, knitr,
               GGally, ggcorrplot, tidymodels,
               randomForest, Rcpp, ranger,
               kableExtra, tree, rpart, caret,
               VSURF)
experiment <- FALSE
```

There are 515345 observations in the dataset. 90 attributes describe each and single observation. All of the attributes are real number and correspond to the timbre either average or covariance. The Echo Nest API is used in order to extract the features from the timbre which must be an additional service. In overall, timbre-related features describes spectogram patches. Considering the particular dimensions of the 12-dimension timbre vector they represent the average loudness of the segment, with the second the brightness is described, the third relate to the flatness of a sound and the fourth to the sounds with stronger attack [@Bertin-Mahieux2013].


## Data Preprocessing

## Reading data
```{r, echo = FALSE}
source('..\\src\\utils\\fit_table.r')
```

```{r}
if (experiment){
  raw_data <- read.csv2('..\\data\\raw\\YearPredictionMSD_sample.txt')
} else{
  raw_data <- read.csv('..\\data\\raw\\YearPredictionMSD.txt', header = FALSE)
}
raw_data %>% fit_table()
```

The first column in the dataset correspond to the year that will be predicted from the features. We rename it for the sake of the modelling purposes. With the MSD many tasks can be addressed, however the song release year brings many practical applications and there are not a lot of research papers that relate to this prediction problem. Only audio features are used in this problem which is justified with the fact that listeners are often fond of music from certain periods of their lives. Another reason might be the evolution of timbre across years.

## Renaming columns

```{r}
raw_data <- raw_data %>%
        rename(year = V1) %>%
        mutate(year = as.numeric(year))
raw_data %>% fit_table()
```

In the next step we split the dataset into the attributes and the target variable. Thanks to that we will not mix the attributes in the feature engineering process.

```{r}
target_data <- raw_data %>% select(year)
```
## Scaling

The authors in the main article mapped the year values using linear mapping to [0,1] range [@Bertin-Mahieux2011]. We follow the convention in the analysis below.

```{r}
# mapping target to [0,1] range
range_to_0_1 <- function(vec) {
  (vec - min(vec)) / (max(vec) - min(vec))
}

raw_data$year_scaled <- range_to_0_1(raw_data$year)
raw_data$year_scaled <- range_to_0_1(raw_data$year)
```

## Splitting dataset

The result might suggest that the dataset is somehow not completely random. It is confirmed with the fact that the following train / test split should be respected. First 463 715 observations should be used for the training purposes while the remaining 51 630 are the observations on which the regression model will be tested.

```{r}
if (experiment){
  train_data <- raw_data %>% slice_head(n = 890) %>% select(-year)
  test_data <- raw_data %>% slice_tail(n = 110)  %>% select(-year)
} else{
  train_data <- raw_data %>% slice_head(n = 463715)  %>% select(-year)
  test_data <- raw_data %>% slice_tail(n = 51630) %>% select(-year)
}
```

The dataset authors add that thanks to that split 'producer effect' will be avoided.
Next, we confirm that the remaining attributes correspond to the timbre covariates.

```{r}
timbre_average_data <- raw_data %>%
        select(V2:V13)
timbre_average_data %>% fit_table()
```
We needed to investigate what the timbre in the sound analysis actually means. It is described as everything that can be differentiated between two different tracks by the listener having kept the same pitch, spatial location and loudness and refers to the perceptual quality of a sound and color. "Brightness", "clarity", "harshness", "fullness" and "noisiness" are frequently used adjectives that are associated with the timbre word [@Allen2018].

```{r}
timbre_covariance_data <- raw_data %>%
        select(V14:V91)
timbre_covariance_data %>% fit_table()
```

## Handling missing values

```{r}
raw_data %>%
        summarise(across(everything(), ~sum(is.na(.))))
```
There are no missing values across columns and in the whole dataset. This is an important insight that will allow for an easier later phases of the project e.g exploratory data analysis and modelling.

# Descriptive analyses of the data

Having described the dataset we can follow the analysis and in the next points visualized dataset including both target and features on which the model will be trained.

## One-dimensional plots

The target variable in the problem analyzed is year. We easily spot the distribution of it being heavily skewed.

```{r}
raw_data %>%
    ggplot(aes(x = year)) +
    geom_histogram(aes(y = ..density..),
                   bins = 50, alpha = 0.4, fill = "pink", col = "gray30") +
    geom_density(alpha = .2, fill = "blueviolet") +
    labs(title = "Distribution of the outcome variable - density and histogram",
         caption = "source: produced with ggplot package") +
    theme(plot.title = element_text(hjust = 0.5, size = 20),
          axis.text.x = element_text(size = 20))
```

## Two-dimensional plots

### Number of songs across centuries
For the first plot we have decided to squeeze the year variable based on the century because the distribution of the songs across years is highly nonuniform.
```{r}
timbre_average_data %>%
    mutate(year = target_data$year) %>%
    mutate(century = if_else(year <= 2000, 'XX', 'XXI')) %>%
    group_by(century) %>%
    count() %>%
    ggplot(aes(x = century, y = n)) +
    geom_bar(stat = "identity") +
    ggtitle('The songs count across centuries') +
    labs(caption = "source: produced with ggplot package",
         x = 'Century',
         y = 'Count') +
    theme_minimal() +
    theme(legend.position = 'bottom')  +
    guides(fill = guide_legend(nrow = 2,byrow = TRUE)) +
    scale_fill_grey() +
    theme(plot.title = element_text(hjust = 0.5, size = 20))
```

We see that the count of songs increase with the centuries, although, there are more years in the dataset, in XX century.

### Loudness across decades

```{r}
timbre_average_data %>%
    mutate(year = target_data %>% pull(year)) %>%
    mutate(decade = as.factor(floor(year/10)*10)) %>%
    rename(loudness = V2) %>%
    select(loudness, decade) %>%
    ggplot(aes(x= decade, y = loudness, group=decade, fill = decade)) +
    geom_boxplot() +
    labs(title = "Loudness across decades") +
    guides(fill = "none", group = "none") +
    theme(plot.title = element_text(hjust = 0.5))
```

Increase in loudness is spotted across decades. Nowadays people listen to louder music more.

### Correlation matrix
An important area that should not be neglected in the process of building machine learning model is creating the correlation matrix based on which we will continue investigation regarding the two-dimensional plots. We have decided to learn more about the timbre split suggested by the authors. Based on that we can check whether average timbre or timbre covariance have stronger influence on the outcome variable.

```{r}
corr_average <- timbre_average_data %>%
        mutate(year = target_data$year) %>%
        cor()

ggcorrplot(corr_average, hc.order = TRUE, type = "lower", lab = TRUE) +
    labs(title = "Correlation between timbre average data and the outcome variable.") +
    theme(plot.title = element_text(hjust = 0.5, size = 20))
```

Based on the correlation matrix we spot two variables that might be important in predicting the song release year V2 and V7. V7 is negatively correlated with the year which means that the higher the variable the lower the year.

```{r}
corr_covariance <- timbre_covariance_data %>%
        mutate(year = target_data$year) %>%
        cor()
```

```{r}
high_corr_columns <- corr_covariance %>%
        as_tibble() %>%
        mutate(row_name = c(timbre_covariance_data %>% names(), "year")) %>%
        relocate(row_name) %>%
        pivot_longer(-row_name) %>%
        filter(name == 'year') %>%
        filter(value > 0.05) %>%
        pull(row_name)
```


Because there are a lot of columns inside the timbre covariance matrix we choose to visualize only the ones that are greater than chosen level of correlation. Interestingly it is set very low to 0.05 since the association is extremely low.

```{r}
corr_covariance <-  timbre_covariance_data %>%
        mutate(year = target_data$year) %>%
        select(all_of(high_corr_columns)) %>%
        cor()
```

```{r}
ggcorrplot(corr_covariance,
           hc.order = TRUE,
           type = "lower",
           lab = TRUE,
           method = 'circle',
           lab_size = 3) +
  labs(title = "Correlation between timbre covariance data and the outcome variable.") +
  theme(plot.title = element_text(hjust = 0.5, size = 20))
```

The highest correlation with the target variable is spotted between variable V21, V74, V15 and V40 and year. What is more the multicollinearity might be spotted for variable V15 and V21 with the correlation value 0.48.

## Statistical analysis
```{r}
source("..\\src\\utils\\t_test_across_centuries.R")
t_test_test_across_centuries(timbre_average_data %>%
                                     mutate(year = target_data$year),
                             'V2')
t_test_test_across_centuries(timbre_average_data %>%
                                     mutate(year = target_data$year),
                             'V7')
t_test_test_across_centuries(timbre_covariance_data %>%
                                     mutate(year = target_data$year),
                             'V21')
t_test_test_across_centuries(timbre_covariance_data %>%
                                     mutate(year = target_data$year),
                             'V15')
```
We have show that across the centuries averages differ for the variables that are the most correlated with the target variable. It might be an indicator that these variables will play a significant role in predicting the song release year.

# Variable transformation

## Interactions
```{r}
first_model_formula <- year_scaled ~ V2 + V7 + V21 + V15

rec <- recipe(first_model_formula, data = train_data )

int_mod_1 <- rec %>%
  step_interact(terms = ~ V2:starts_with("V"))

int_mod_1 <- prep(int_mod_1, training = train_data)

transformed_data <- bake(int_mod_1, train_data)
```
# Variable selection

We investigated two packages varSelRF [@varselrf] and VSURF [@genuer:hal-01251924] for variable selection step. We have chosen
them mainly because of their dependency on the randomForest package that was used during the classes. The latter one
is not implemented for regression problem. Hence, varSelfRF will be used which is more complicated procedure. It
consists of three steps: thresholding, interpretation and prediction step. Thresholding is based on variable importance.
The interpretation embeds random forest models starting with the one built only on the variables chosen in the previous step.
Next, the stepwise selection procedure is applied in the final, prediction procedure.

```{r}
vsurf <- VSURF(x = dat_1 %>% select(-year_scaled), y = dat_1$year_scaled,
               ntree = 100, nfor.thres = 20,
               nfor.interp = 10, nfor.pred = 10)
```
The method already trains the first model. We will evaluate its performance during the comparison of the trained machine learning models.

# Training/test data division and resampling

## tree model

We would like to understand at this level of aggregation their level of contribution to the overall score.

```{r}
cov_tree <- tree(first_model_formula, transformed_data)
```
```{r}
summary(cov_tree)
```
```{r}
plot(cov_tree)
text(cov_tree, pretty = 0)
```
```{r}
summary(cov_tree)
```

Controlling for size of trees.

```{r}
rpart_tree <- rpart(first_model_formula,
                     data = transformed_data,
                     method = "anova")
plotcp(rpart_tree)
```

Following **the rule of 1-SE** we choose trees with 14 terminal nodes. Thanks to that overfitting can be omitted.

Other values can be optimized in the process as well `minsplit` and `maxdepth`. The first one relates to the number of observations that need to belong to the node before the split is considered. The latter one describe the height of the tree. It is the number of nodes between the root and the deepest terminal node.

```{r, results='hide'}
hyper_grid <- expand.grid(
        minsplit = seq(5, 20, 1),
        maxdepth = seq(8, 15, 1)
)
```
```{r, results='hide'}
models <- list()
for (i in 1:nrow(hyper_grid)) {

  cat(i, "/", nrow(hyper_grid), "\n", sep = "")

  # setting the values
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]

  # settin the seed
  set.seed(123123 + i)

  # training of the model and saving results to the list
  models[[i]] <- rpart(
          formula = first_model_formula,
          data    = train_data,
          method  = "anova",
          control = list(minsplit = minsplit, maxdepth = maxdepth)
  )
}
```

```{r }
get_cp <- function(x) {
  min <- which.min(x$cptable[, "xerror"])
  cp  <- x$cptable[min, "CP"]
  return(cp)
}

get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"]
  return(xerror)
}
```
```{r, echo = FALSE}
hyper_grid %>%
        mutate(
                cp    = purrr::map_dbl(models, get_cp),
                error = purrr::map_dbl(models, get_min_error)
        ) %>%
        arrange(error) %>%
        top_n(-5, wt = error)
```

Let's examine performance of the model with the lowest prediction error
on the testing set.

```{r }
optimal_tree <- rpart(
        formula = first_model_formula,
        data    = train_data,
        method  = "anova",
        control = list(minsplit = 14, maxdepth = 12, cp = 0.01)
)

pred <- predict(optimal_tree, newdata = test_data)
optimal_RMSE <- RMSE(pred = pred, obs = test_data$year_scaled)
```

```{r }
tc      <- trainControl(method = "cv", number = 10)
cp.grid <- expand.grid(cp = seq(0, 0.03, 0.001))
```

```{r}
tree3 <- train(first_model_formula,
                      data = train_data,
                      method = "rpart",
                      trControl = tc,
                      tuneGrid = cp.grid)

pred <- predict(tree3, newdata = test_data)

tree3_RMSE <- RMSE(pred = pred, obs = test_data$year_scaled)
```

## Bagging and random forest

Firstly, we run `randomForest()` function. It does not allow for missing values in the dataset. However, as previously shown we do not deal with this problem in our dataset.

```{r}
rf <- randomForest(first_model_formula,
                           data = train_data)
```
Interestingly, for random forest the prediction error can be computed without applying the model on the test dataset. The out-of-bag error is estimated then. Based on the stochastic experiment performed during the classes, the OOB is around 63 % of the size of the original data set.

```{r}
print(rf)
```
```{r}
plot(rf)
```
Some of the advantages of randomForest package refer to its simplicity. Above we can see two efficient ways of accessing object metadata. Both ways are presented, the graphical and the tabular form. What is more the graphical representation of the most influencial variables can be achieved very easily.

```{r}
varImpPlot(rf,
           sort = TRUE,
           main = "Importance of predicors")
```

The `mtry` parameter can be optimized with the cross-validation process.

```{r}
parameters_rf <- expand.grid(mtry = 2:15)
ctrl_oob <- trainControl(method = "oob")
rf3 <- train(first_model_formula,
              data = train_data,
              method = "rf",
              ntree = 100,
              nodesize = 100,
              tuneGrid = parameters_rf,
              trControl = ctrl_oob,
              importance = TRUE)
```

With that object the analysis can be performed efficiently.

## XGBoost

```{r}
xgb_parameters <-
        expand.grid(nrounds = 320,
                    max_depth = 9,
                    eta = 0.06,
                    gamma = 1,
                    colsample_bytree = 0.7,
                    min_child_weight = 200,
                    subsample = 0.9)
xgb_model <-
        train(first_model_formula,
              data = train_data,
              method = "xgbTree",
              preProcess = c("center", "scale"),
              tuneGrid  = xgb_parameters)
```

# Comparison
```{r}
source('..\\src\\models\\getRegressionMetrics.R')
```
```{r }
bind_cols(tibble(tree         =  predict(tree3,  test_data),
                 optimal_tree         =  predict(optimal_tree,  test_data))) %>%
        map_dfr(getRegressionMetrics, real = test_data$year_scaled, .id = "model")
```

# Summary and conclusions

First of all, for the first time we have been modelling on dataset of this size.
We needed to take appropriate measures in order to experiment faster for instance
sampling 1000 observations and working only on them.

Secondly, the variables' names were not provided and we managed to find the naming for only few of them. We were not aware how the timbre might influence the outcome variable and haven't any intuition towards the variables that might be important. Hence, we focused on quantitative measures such as correlations or statistical tests. Nevertheless, the visualizations also occurred to be informative.

What is more, the variable selection process was an opportunity to learn more about
how to choose packages. During the notebook production we have already known
what libraries were introduced during the classes (randomForest), what machine learning problem we analyze (regression) and what are the methods that it needs to implement (predict). Considering these three constraints we have chosen one that suits all of the aforementioned needs.

Last but not least, it has been a fruitful experience to have all the results
stored in a single table and compare them easily. Based on them we see that how
the performance measures are distributed across models.

# References

# Appendix

```{r}
source('..\\src\\utils\\fit_rf_model_with_recipe.R')
simple_recipe_with_log <-
        recipe(year_scaled ~ .,
               data = train_data %>% mutate(year = target_data$year_scaled)) %>%
                step_log(V15, base = 10)
```


```{r}
simple_recipe_with_interact <-
        recipe(year_scaled ~ .,
               data = train_data) %>%
                step_interact(terms = ~V14:V91)
```

## Model with transformed variables
```{r}
fit_rf_model_with_recipe(simple_recipe_with_log, train_data)

fit_rf_model_with_recipe(simple_recipe_with_interact, train_data)
```